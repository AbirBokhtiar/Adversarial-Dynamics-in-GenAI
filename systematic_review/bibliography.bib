@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014},
  url={https://arxiv.org/abs/1412.6572},
  note={Foundational work on adversarial examples in deep learning}
}

@inproceedings{szegedy2014intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2014},
  url={https://arxiv.org/abs/1312.6199},
  note={First discovery of adversarial examples in neural networks}
}

@article{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  journal={IEEE Symposium on Security and Privacy (SP)},
  pages={39--57},
  year={2017},
  publisher={IEEE},
  url={https://arxiv.org/abs/1608.04644},
  note={Developed C\&W attack, one of the strongest adversarial attacks}
}

@article{brown2017adversarial,
  title={Adversarial patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n and Gilmer, Justin},
  journal={arXiv preprint arXiv:1712.09665},
  year={2017},
  url={https://arxiv.org/abs/1712.09665},
  note={Physical-world adversarial examples using patches}
}

@article{athalye2018synthesizing,
  title={Synthesizing robust adversarial examples},
  author={Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
  journal={International Conference on Machine Learning (ICML)},
  pages={284--293},
  year={2018},
  url={https://arxiv.org/abs/1707.07397},
  note={3D adversarial examples robust to transformations}
}

@article{madry2018towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018},
  url={https://arxiv.org/abs/1706.06083},
  note={PGD attack and adversarial training for robustness}
}

@article{moosavi2017universal,
  title={Universal adversarial perturbations},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={1765--1773},
  year={2017},
  url={https://arxiv.org/abs/1610.08401},
  note={Image-agnostic perturbations that fool classifiers}
}

@article{ilyas2019adversarial,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={32},
  year={2019},
  url={https://arxiv.org/abs/1905.02175},
  note={Challenges conventional understanding of adversarial examples}
}

@article{kurakin2017adversarial,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  journal={International Conference on Learning Representations (ICLR) Workshop},
  year={2017},
  url={https://arxiv.org/abs/1607.02533},
  note={Physical adversarial examples robust to transformations}
}

@article{papernot2016transferability,
  title={Transferability in machine learning: from phenomena to black-box attacks using adversarial samples},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian},
  journal={arXiv preprint arXiv:1605.07277},
  year={2016},
  url={https://arxiv.org/abs/1605.07277},
  note={Transferability of adversarial examples across models}
}

@article{biggio2013evasion,
  title={Evasion attacks against machine learning at test time},
  author={Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and {\v{S}}rndi{\'c}, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
  journal={Joint European conference on machine learning and knowledge discovery in databases},
  pages={387--402},
  year={2013},
  publisher={Springer},
  url={https://arxiv.org/abs/1708.06131},
  note={Early work on evasion attacks in machine learning}
}

@article{papernot2017practical,
  title={Practical black-box attacks against machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
  journal={ACM on Asia Conference on Computer and Communications Security},
  pages={506--519},
  year={2017},
  url={https://arxiv.org/abs/1602.02697},
  note={Black-box adversarial attacks using model transferability}
}

@article{tramèr2020ensemble,
  title={Ensemble adversarial training: Attacks and defenses},
  author={Tram{\`e}r, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018},
  url={https://arxiv.org/abs/1705.07204},
  note={Defense mechanism using ensemble adversarial training}
}

@article{weng2018towards,
  title={Towards fast computation of certified robustness for relu networks},
  author={Weng, Tsui-Wei and Zhang, Huan and Chen, Hongge and Song, Zhao and Hsieh, Cho-Jui and Boning, Duane and Dhillon, Inderjit S and Daniel, Luca},
  journal={International Conference on Machine Learning (ICML)},
  pages={5276--5285},
  year={2018},
  url={https://arxiv.org/abs/1804.09699},
  note={Certified defense against adversarial attacks}
}

@article{raghunathan2018certified,
  title={Certified defenses against adversarial examples},
  author={Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018},
  url={https://arxiv.org/abs/1801.09344},
  note={Provable defenses with certified bounds}
}

@article{song2018pixeldefend,
  title={PixelDefend: Leveraging generative models to understand and defend against adversarial examples},
  author={Song, Yang and Kim, Taesup and Nowozin, Sebastian and Ermon, Stefano and Kushman, Nate},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018},
  url={https://arxiv.org/abs/1710.10766},
  note={Defense using generative models to purify adversarial examples}
}

@article{samangouei2018defense,
  title={Defense-GAN: Protecting classifiers against adversarial attacks using generative models},
  author={Samangouei, Pouya and Kabkab, Maya and Chellappa, Rama},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018},
  url={https://arxiv.org/abs/1805.06605},
  note={GAN-based defense mechanism against adversarial attacks}
}

@article{shan2023prompt,
  title={Prompt injection attacks and defenses in LLM-integrated applications},
  author={Shan, Yi and Hamilton, Brent A and Hahn, Sven and Zamanian, Soroush and Kıcıman, Emre and Kapoor, Ashish},
  journal={arXiv preprint arXiv:2310.12815},
  year={2023},
  url={https://arxiv.org/abs/2310.12815},
  note={Adversarial attacks specific to large language models}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={3419--3448},
  year={2022},
  url={https://arxiv.org/abs/2202.03286},
  note={Automated red teaming for language models}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023},
  url={https://arxiv.org/abs/2307.15043},
  note={Universal adversarial suffixes for LLMs}
}

@article{wei2023jailbroken,
  title={Jailbroken: How does LLM safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  url={https://arxiv.org/abs/2307.02483},
  note={Analysis of jailbreaking attacks on aligned LLMs}
}

@article{wallace2019universal,
  title={Universal adversarial triggers for attacking and analyzing NLP},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  journal={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2153--2162},
  year={2019},
  url={https://arxiv.org/abs/1908.07125},
  note={Universal triggers for NLP models}
}

@article{ebrahimi2018hotflip,
  title={HotFlip: White-box adversarial examples for text classification},
  author={Ebrahimi, Javid and Rao, Anyi and Lowd, Daniel and Dou, Dejing},
  journal={Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={31--36},
  year={2018},
  url={https://arxiv.org/abs/1712.06751},
  note={Character-level adversarial attacks on text classifiers}
}

@article{gao2018black,
  title={Black-box generation of adversarial text sequences to evade deep learning classifiers},
  author={Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
  journal={IEEE Security and Privacy Workshops (SPW)},
  pages={50--56},
  year={2018},
  url={https://arxiv.org/abs/1801.04354},
  note={Black-box adversarial attacks on text classifiers}
}

@article{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  journal={USENIX Security Symposium},
  pages={2633--2650},
  year={2021},
  url={https://arxiv.org/abs/2012.07805},
  note={Privacy attacks extracting training data from LLMs}
}

@article{shayegani2023jailbreak,
  title={Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models},
  author={Shayegani, Erfan and Mamun, Md Abdullah Al and Fu, Yu and Zaree, Pedram and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2307.14539},
  year={2023},
  url={https://arxiv.org/abs/2307.14539},
  note={Adversarial attacks on multi-modal language models}
}

@article{qi2023visual,
  title={Visual adversarial examples jailbreak aligned large language models},
  author={Qi, Xiangyu and Huang, Kaixuan and Panda, Ashish and Wang, Mengdi and Mittal, Prateek},
  journal={arXiv preprint arXiv:2306.13213},
  year={2023},
  url={https://arxiv.org/abs/2306.13213},
  note={Visual adversarial examples for multi-modal LLMs}
}

@article{liu2023jailbreaking,
  title={Jailbreaking ChatGPT via prompt engineering: An empirical study},
  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2305.13860},
  year={2023},
  url={https://arxiv.org/abs/2305.13860},
  note={Systematic study of jailbreaking techniques for ChatGPT}
}

@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023},
  url={https://arxiv.org/abs/2310.08419},
  note={Efficient jailbreaking with minimal queries}
}

@article{deng2023multilingual,
  title={Multilingual jailbreak challenges in large language models},
  author={Deng, Yue and Zhang, Wenxuan and Pan, Sinno Jialin and Bing, Lidong},
  journal={arXiv preprint arXiv:2310.06474},
  year={2023},
  url={https://arxiv.org/abs/2310.06474},
  note={Cross-lingual jailbreaking attacks on LLMs}
}

@article{shen2023anything,
  title={``Do anything now'': Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2308.03825},
  year={2023},
  url={https://arxiv.org/abs/2308.03825},
  note={Characterization of real-world jailbreak prompts}
}

@article{huang2023catastrophic,
  title={Catastrophic jailbreak of open-source LLMs via exploiting generation},
  author={Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06987},
  year={2023},
  url={https://arxiv.org/abs/2310.06987},
  note={Generation-based jailbreak attacks on open-source LLMs}
}

@article{yu2023gptfuzzer,
  title={GPTFuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Wu, Xingwei and Qu, Xinyu and Li, Yue and Zhao, Zhiming and Liu, Zeyu and Cheng, Yang},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023},
  url={https://arxiv.org/abs/2309.10253},
  note={Automated fuzzing-based jailbreak generation}
}

@article{liu2023autodan,
  title={AutoDAN: Generating stealthy jailbreak prompts on aligned large language models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2310.04451},
  year={2023},
  url={https://arxiv.org/abs/2310.04451},
  note={Hierarchical genetic algorithm for jailbreak generation}
}

@article{wallace2024instruction,
  title={Instruction hierarchy: Training LLMs to prioritize privileged instructions},
  author={Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex},
  journal={arXiv preprint arXiv:2404.13208},
  year={2024},
  url={https://arxiv.org/abs/2404.13208},
  note={Defense mechanism against prompt injection attacks}
}

@article{robey2023smoothllm,
  title={SmoothLLM: Defending large language models against jailbreaking attacks},
  author={Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2310.03684},
  year={2023},
  url={https://arxiv.org/abs/2310.03684},
  note={Randomized smoothing defense for LLMs}
}

@article{jain2023baseline,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Pinyuan and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023},
  url={https://arxiv.org/abs/2309.00614},
  note={Evaluation of defense strategies for LLM safety}
}

@article{zhang2024defending,
  title={Defending large language models against jailbreaking attacks through goal prioritization},
  author={Zhang, Zhexin and Zhao, Junxiao and Wang, Pei and Yang, Fangzhao and Liu, Hongwei and Hong, Chuhan and Gao, Shangdi and Xie, Xing},
  journal={arXiv preprint arXiv:2311.09096},
  year={2024},
  url={https://arxiv.org/abs/2311.09096},
  note={Defense based on goal prioritization in LLMs}
}

@article{zou2024improving,
  title={Improving alignment and robustness with circuit breakers},
  author={Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek and Lin, Maxwell and Andriushchenko, Maksym and Croce, Francesco and Barez, Fazl and Kolter, J Zico and Fredrikson, Matt and Hendrycks, Dan},
  journal={arXiv preprint arXiv:2406.04313},
  year={2024},
  url={https://arxiv.org/abs/2406.04313},
  note={Circuit breaker approach for robust LLM alignment}
}

@article{gehman2020realtoxicityprompts,
  title={RealToxicityPrompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3356--3369},
  year={2020},
  url={https://arxiv.org/abs/2009.11462},
  note={Benchmark for evaluating toxicity in language models}
}

@article{perez2022discovering,
  title={Discovering language model behaviors with model-written evaluations},
  author={Perez, Ethan and Ringer, Sam and Luk{\'o}{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and others},
  journal={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={13387--13434},
  year={2023},
  url={https://arxiv.org/abs/2212.09251},
  note={Automated evaluation of LLM behaviors}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022},
  url={https://arxiv.org/abs/2209.07858},
  note={Comprehensive study on red teaming methodologies}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022},
  url={https://arxiv.org/abs/2212.08073},
  note={Self-improvement approach for LLM safety}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={35},
  pages={27730--27744},
  year={2022},
  url={https://arxiv.org/abs/2203.02155},
  note={InstructGPT: RLHF for instruction following}
}

@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022},
  url={https://arxiv.org/abs/2209.14375},
  note={Sparrow: Dialogue agent with improved alignment}
}

@article{dubey2024llama,
  title={The Llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024},
  url={https://arxiv.org/abs/2407.21783},
  note={Llama 3 models with improved safety mechanisms}
}

@article{achiam2023gpt,
  title={GPT-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023},
  url={https://arxiv.org/abs/2303.08774},
  note={GPT-4 with enhanced safety features}
}

@article{anil2023gemini,
  title={Gemini: A family of highly capable multimodal models},
  author={Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and Silver, David and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023},
  url={https://arxiv.org/abs/2312.11805},
  note={Multi-modal model with safety considerations}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023},
  url={https://arxiv.org/abs/2307.09288},
  note={Open-source LLM with safety fine-tuning}
}
