Study ID,First Author,Publication Year,Title,Publication Type,Publication Venue,DOI/URL,Study Design,Research Question/Objective,Target Model Type,Model Architecture,Domain/Application,Attack Type,Attack Method,Attack Success Rate,Defense Mechanism,Defense Effectiveness,Evaluation Metrics,Dataset Used,Sample Size,Key Findings,Limitations,Future Work Recommendations,Relevance to Adversarial Dynamics,Quality Score (1-5),Notes
S001,Goodfellow,2014,Explaining and harnessing adversarial examples,Journal Article,arXiv preprint,https://arxiv.org/abs/1412.6572,Theoretical & Empirical,Explain adversarial examples and propose FGSM attack,Image Classifiers,Deep Neural Networks,Computer Vision,White-box Gradient-based,Fast Gradient Sign Method (FGSM),High (>90%),Adversarial Training,Moderate,Accuracy under attack,MNIST/CIFAR-10,N/A,Linear behavior explains adversarial examples,Limited to simple models,Explore non-linear models,High - foundational work,5,Foundational paper on adversarial examples
S002,Szegedy,2014,Intriguing properties of neural networks,Conference Paper,ICLR,https://arxiv.org/abs/1312.6199,Empirical,Discover adversarial examples in neural networks,Image Classifiers,Convolutional Neural Networks,Computer Vision,White-box Optimization-based,L-BFGS optimization,High (>95%),None,N/A,Misclassification rate,ImageNet,Multiple samples,Small perturbations cause misclassification,Limited defense analysis,Develop robust models,High - first discovery,5,First paper discovering adversarial examples
S003,Carlini,2017,Towards evaluating the robustness of neural networks,Conference Paper,IEEE S&P,https://arxiv.org/abs/1608.04644,Empirical & Analytical,Develop strong adversarial attacks (C&W),Image Classifiers,Deep Neural Networks,Computer Vision,White-box Optimization-based,Carlini & Wagner (C&W) attack,Very High (>99%),Defensive Distillation,Bypassed,L0/L2/L∞ distance,MNIST/CIFAR-10/ImageNet,10000+,C&W attack defeats existing defenses,Computationally expensive,Certified defenses needed,High - strongest attack,5,Demonstrated inadequacy of existing defenses
S004,Brown,2017,Adversarial patch,Journal Article,arXiv preprint,https://arxiv.org/abs/1712.09665,Empirical,Create physical adversarial patches,Object Detectors,CNNs,Computer Vision,Physical-world,Adversarial Patch,Moderate to High (varies),None tested,N/A,Detection accuracy,ImageNet,Multiple objects,Physical patches fool classifiers,Limited to specific objects,Improve patch robustness,High - physical attacks,4,Practical real-world attack scenario
S005,Athalye,2018,Synthesizing robust adversarial examples,Conference Paper,ICML,https://arxiv.org/abs/1707.07397,Empirical,Create 3D adversarial examples,Object Classifiers,Deep Neural Networks,Computer Vision,Physical-world,Expectation over Transformation (EOT),High (>80%),None tested,N/A,Accuracy across transformations,Various datasets,Multiple 3D objects,3D examples robust to transformations,Limited to specific viewpoints,Scale to diverse scenarios,High - robust physical attacks,5,Demonstrates persistent adversarial examples
S006,Madry,2018,Towards deep learning models resistant to adversarial attacks,Conference Paper,ICLR,https://arxiv.org/abs/1706.06083,Theoretical & Empirical,Develop PGD attack and adversarial training,Image Classifiers,Deep Neural Networks,Computer Vision,White-box Iterative,Projected Gradient Descent (PGD),Very High (>99%),Adversarial Training,Good (50-60% robust accuracy),Robust accuracy,MNIST/CIFAR-10,60000/50000,PGD is strongest first-order attack,Computational cost high,Explore certified defenses,High - strong defense baseline,5,Gold standard for adversarial robustness
S007,Moosavi-Dezfooli,2017,Universal adversarial perturbations,Conference Paper,CVPR,https://arxiv.org/abs/1610.08401,Empirical,Find universal perturbations,Image Classifiers,CNNs,Computer Vision,White-box Optimization,Universal perturbation algorithm,High (>80%),None tested,N/A,Fooling rate,ImageNet,50000,Single perturbation fools most images,Limited transferability across models,Understand universal properties,High - universal attacks,5,Image-agnostic adversarial perturbations
S008,Ilyas,2019,Adversarial examples are not bugs they are features,Conference Paper,NeurIPS,https://arxiv.org/abs/1905.02175,Theoretical & Empirical,Explain adversarial examples as features,Image Classifiers,ResNet,Computer Vision,Theoretical,Feature analysis,N/A,None,N/A,Model robustness/accuracy,CIFAR-10/ImageNet,50000+,Adversarial examples exploit useful features,Limited to image domain,Extend to other domains,High - paradigm shift,5,Challenges conventional understanding
S009,Kurakin,2017,Adversarial examples in the physical world,Workshop Paper,ICLR Workshop,https://arxiv.org/abs/1607.02533,Empirical,Test physical adversarial examples,Image Classifiers,Inception,Computer Vision,Physical-world,Printed adversarial images,Moderate (40-60%),None,N/A,Classification accuracy,ImageNet,Multiple images,Physical perturbations persist,Limited by printing process,Improve physical robustness,High - real-world applicability,4,Demonstrates practical threat
S010,Papernot,2016,Transferability in machine learning,Journal Article,arXiv preprint,https://arxiv.org/abs/1605.07277,Empirical,Study adversarial transferability,Image/Text Classifiers,Various DNNs,Multiple domains,Black-box Transfer,Substitute model training,Moderate to High (varies),None tested,N/A,Transfer success rate,MNIST/CIFAR-10,Multiple samples,Adversarial examples transfer across models,Limited understanding of mechanism,Theoretical analysis needed,High - enables black-box attacks,5,Critical for black-box attack scenarios
S011,Biggio,2013,Evasion attacks against machine learning at test time,Conference Paper,ECML PKDD,https://arxiv.org/abs/1708.06131,Theoretical & Empirical,Study evasion attacks on ML,Various Classifiers,SVM/Neural Networks,Malware Detection,White-box Gradient-based,Gradient-based evasion,High (>70%),None,N/A,Evasion rate,PDF malware,Multiple samples,Test-time attacks effective,Limited to specific domain,General framework needed,Medium - early work,4,Early work on adversarial ML
S012,Papernot,2017,Practical black-box attacks against machine learning,Conference Paper,ACM ASIACCS,https://arxiv.org/abs/1602.02697,Empirical,Develop practical black-box attacks,Image Classifiers,DNNs,Computer Vision,Black-box,Substitute model attack,High (>80%),None tested,N/A,Attack success rate,MNIST/CIFAR-10/GTSRB,Multiple samples,Black-box attacks via transferability,Requires query access,Improve query efficiency,High - practical attack,5,Enables practical black-box attacks
S013,Tramèr,2018,Ensemble adversarial training,Conference Paper,ICLR,https://arxiv.org/abs/1705.07204,Empirical,Improve adversarial training,Image Classifiers,ResNet,Computer Vision,White-box,Multiple attacks,N/A,Ensemble Adversarial Training,Good improvement,Robust accuracy,MNIST/CIFAR-10/ImageNet,Multiple samples,Ensemble training improves robustness,Still vulnerable to strong attacks,Explore certified methods,High - improved defense,4,Practical defense improvement
S014,Weng,2018,Towards fast computation of certified robustness,Conference Paper,ICML,https://arxiv.org/abs/1804.09699,Theoretical & Empirical,Fast certified robustness computation,Image Classifiers,ReLU Networks,Computer Vision,Certification,Linear programming relaxation,N/A,Certified Defense,Provable guarantees,Certified radius,MNIST/CIFAR-10,10000,Fast certification algorithm,Limited to small networks,Scale to large networks,High - certified defense,5,Efficient certified robustness
S015,Raghunathan,2018,Certified defenses against adversarial examples,Conference Paper,ICLR,https://arxiv.org/abs/1801.09344,Theoretical & Empirical,Provide certified defenses,Image Classifiers,Neural Networks,Computer Vision,Certification,Semidefinite programming,N/A,Certified Defense,Provable bounds,Certified accuracy,MNIST,10000,Provable defense with guarantees,Scalability issues,Improve scalability,High - theoretical guarantee,5,Provable defense mechanism
S016,Song,2018,PixelDefend,Conference Paper,ICLR,https://arxiv.org/abs/1710.10766,Empirical,Use generative models for defense,Image Classifiers,PixelCNN,Computer Vision,Defense Mechanism,Generative model purification,N/A,PixelDefend,Good (>80% recovery),Classification accuracy,CIFAR-10/Fashion-MNIST,10000,Purification effective against attacks,Adaptive attacks can bypass,Robust purification methods,Medium - can be bypassed,4,Generative model-based defense
S017,Samangouei,2018,Defense-GAN,Conference Paper,ICLR,https://arxiv.org/abs/1805.06605,Empirical,GAN-based defense mechanism,Image Classifiers,GAN,Computer Vision,Defense Mechanism,GAN-based reconstruction,N/A,Defense-GAN,Moderate effectiveness,Classification accuracy,MNIST/F-MNIST,Multiple samples,GANs can defend against attacks,Limited to distribution,Improve GAN robustness,Medium - limited effectiveness,4,GAN-based defense approach
S018,Shan,2023,Prompt injection attacks and defenses,Journal Article,arXiv preprint,https://arxiv.org/abs/2310.12815,Empirical & Analytical,Study prompt injection in LLMs,Large Language Models,Transformer-based,NLP Applications,Prompt Injection,Various injection techniques,High (>70%),Input sanitization,Partial effectiveness,Success rate of injection,Multiple LLMs,100+ prompts,Prompt injection is significant threat,Limited defense options,Develop robust defenses,High - LLM specific,5,Critical for LLM security
S019,Perez,2022,Red teaming language models with language models,Conference Paper,EMNLP,https://arxiv.org/abs/2202.03286,Empirical,Automated red teaming for LLMs,Large Language Models,Transformer,Natural Language,Automated Red Teaming,LM-based red teaming,Varies by metric,RLHF fine-tuning,Reduces harmful outputs,Toxicity/Bias metrics,Multiple LLMs,1000+ prompts,Automated red teaming scalable,Limited coverage,Improve coverage,High - automated evaluation,5,Scalable red teaming approach
S020,Zou,2023,Universal and transferable adversarial attacks on aligned LLMs,Journal Article,arXiv preprint,https://arxiv.org/abs/2307.15043,Empirical,Develop universal adversarial suffixes,Large Language Models,LLaMA/GPT variants,Natural Language,White-box Optimization,Greedy Coordinate Gradient (GCG),High (>80%),None tested,N/A,Attack success rate,AdvBench,Multiple behaviors,Universal suffixes transfer across models,Requires white-box access,Black-box variants needed,High - universal LLM attack,5,Universal jailbreaking technique
S021,Wei,2023,Jailbroken,Conference Paper,NeurIPS,https://arxiv.org/abs/2307.02483,Analytical & Empirical,Analyze LLM safety training failures,Large Language Models,GPT-family,Natural Language,Jailbreak Analysis,Competing objectives analysis,Varies,Safety training,Partially effective,Jailbreak success rate,Multiple datasets,Multiple models,Safety and capabilities in tension,Incomplete understanding,Better training methods,High - understanding failures,5,Important analysis of alignment
S022,Wallace,2019,Universal adversarial triggers for NLP,Conference Paper,EMNLP,https://arxiv.org/abs/1908.07125,Empirical,Develop universal triggers for NLP,NLP Models,BERT/GPT,Natural Language,White-box Gradient,Universal trigger optimization,High (>90%),None tested,N/A,Attack success rate,Various NLP datasets,Multiple tasks,Universal triggers effective on NLP,Limited transferability,Improve defenses,High - NLP specific,5,Universal adversarial triggers
S023,Ebrahimi,2018,HotFlip,Conference Paper,ACL,https://arxiv.org/abs/1712.06751,Empirical,Character-level adversarial attacks,Text Classifiers,CNN/LSTM,Natural Language,White-box,Character flipping,High (>85%),None tested,N/A,Classification accuracy,AG News/IMDB,Multiple samples,Character-level attacks effective,Limited to classification,Extend to generation,Medium - specific domain,4,Character-level perturbations
S024,Gao,2018,Black-box generation of adversarial text,Conference Paper,IEEE SPW,https://arxiv.org/abs/1801.04354,Empirical,Black-box text adversarial attacks,Text Classifiers,RNN/CNN,Natural Language,Black-box,Genetic algorithm,Moderate (60-70%),None tested,N/A,Attack success rate,IMDB/Fake News,1000+ samples,Black-box attacks on text possible,Semantic preservation issues,Improve naturalness,Medium - black-box text,4,Black-box text attack method
S025,Carlini,2021,Extracting training data from LLMs,Conference Paper,USENIX Security,https://arxiv.org/abs/2012.07805,Empirical,Extract training data from LLMs,Large Language Models,GPT-2/GPT-3,Natural Language,Privacy Attack,Targeted extraction,Moderate (varies),Differential privacy,Reduces leakage,Extraction success rate,Various text corpora,Multiple samples,Training data can be extracted,Scale dependent,Stronger privacy guarantees,High - privacy threat,5,Privacy implications for LLMs
S026,Shayegani,2023,Jailbreak in pieces,Journal Article,arXiv preprint,https://arxiv.org/abs/2307.14539,Empirical,Compositional attacks on multi-modal LLMs,Multi-modal LLMs,CLIP/Flamingo,Vision & Language,Multi-modal,Cross-modal jailbreak,High (>75%),None tested,N/A,Jailbreak success rate,Custom dataset,100+ prompts,Multi-modal attacks effective,Limited to specific models,General defense needed,High - multi-modal specific,5,Multi-modal jailbreaking
S027,Qi,2023,Visual adversarial examples jailbreak aligned LLMs,Journal Article,arXiv preprint,https://arxiv.org/abs/2306.13213,Empirical,Visual jailbreaking of LLMs,Multi-modal LLMs,LLaVA/MiniGPT-4,Vision & Language,Visual Adversarial,Typographic attack,Very High (>90%),None tested,N/A,Jailbreak success rate,Custom harmful prompts,Multiple models,Visual modality bypasses text safety,Safety training insufficient,Cross-modal defense,High - novel attack vector,5,Visual channel vulnerability
S028,Liu,2023,Jailbreaking ChatGPT via prompt engineering,Journal Article,arXiv preprint,https://arxiv.org/abs/2305.13860,Empirical,Systematic jailbreak study,ChatGPT,GPT-3.5/GPT-4,Natural Language,Prompt Engineering,Various prompt techniques,Varies (20-80%),OpenAI updates,Partially effective,Jailbreak success rate,Custom scenarios,100+ prompts,Multiple jailbreak patterns identified,Arms race with defenses,Robust evaluation needed,High - systematic study,5,Comprehensive jailbreak taxonomy
S029,Chao,2023,Jailbreaking black box LLMs in twenty queries,Journal Article,arXiv preprint,https://arxiv.org/abs/2310.08419,Empirical,Efficient black-box jailbreaking,Large Language Models,GPT-family/Claude,Natural Language,Black-box Optimization,PAIR algorithm,High (>80%),None tested,N/A,Query efficiency/success rate,AdvBench,Multiple behaviors,Few queries sufficient for jailbreak,Model-specific tuning needed,Transfer across models,High - query efficient,5,Query-efficient jailbreaking
S030,Deng,2023,Multilingual jailbreak challenges,Journal Article,arXiv preprint,https://arxiv.org/abs/2310.06474,Empirical,Cross-lingual jailbreaking,Large Language Models,GPT/Claude/Bard,Natural Language,Cross-lingual,Low-resource language translation,High (>70%),None tested,N/A,Cross-lingual success rate,XSAFETY benchmark,Multiple languages,Safety training language-specific,Multilingual safety needed,Improve coverage,High - multilingual aspect,5,Cross-lingual safety gaps
S031,Shen,2023,Do anything now,Journal Article,arXiv preprint,https://arxiv.org/abs/2308.03825,Empirical,Characterize real-world jailbreaks,Large Language Models,ChatGPT/GPT-4,Natural Language,Prompt Engineering,In-the-wild jailbreaks,Varies,Continuous updates,Partially effective,Jailbreak taxonomy,6387 jailbreak prompts,6387 prompts,Ten jailbreak patterns identified,Dynamic landscape,Automated detection,High - real-world data,5,Real-world jailbreak analysis
S032,Huang,2023,Catastrophic jailbreak,Journal Article,arXiv preprint,https://arxiv.org/abs/2310.06987,Empirical,Generation-based jailbreak,Open-source LLMs,Vicuna/Falcon/LLaMA,Natural Language,Generation-based,GCG on generation,Very High (>95%),None tested,N/A,Jailbreak success rate,AdvBench,Multiple models,Open models more vulnerable,Alignment gaps,Better alignment methods,High - open-source focus,5,Open-source LLM vulnerability
S033,Yu,2023,GPTFuzzer,Journal Article,arXiv preprint,https://arxiv.org/abs/2309.10253,Empirical,Automated jailbreak fuzzing,Large Language Models,ChatGPT/GPT-4,Natural Language,Fuzzing,Template-based fuzzing,High (>70%),None tested,N/A,Jailbreak diversity/success,Custom templates,Multiple scenarios,Fuzzing generates diverse jailbreaks,Template dependency,Template-free methods,High - automated generation,5,Fuzzing-based jailbreak generation
S034,Liu,2023,AutoDAN,Journal Article,arXiv preprint,https://arxiv.org/abs/2310.04451,Empirical,Stealthy jailbreak generation,Large Language Models,GPT-3.5/GPT-4/LLaMA,Natural Language,Hierarchical Genetic,AutoDAN algorithm,High (>80%),Perplexity filter,Partially effective,Jailbreak success/perplexity,AdvBench,Multiple behaviors,Stealthy jailbreaks bypass filters,Computational cost,Improve efficiency,High - stealthy attacks,5,Low-perplexity jailbreaks
S035,Wallace,2024,Instruction hierarchy,Journal Article,arXiv preprint,https://arxiv.org/abs/2404.13208,Empirical,Defense via instruction prioritization,Large Language Models,GPT-family,Natural Language,Defense Mechanism,Hierarchical training,N/A,Instruction Hierarchy,Good (reduces attacks by 63%),Attack success rate,Custom dataset,Multiple scenarios,Prioritization improves robustness,Not foolproof,Combine with other defenses,High - novel defense,5,Instruction prioritization defense
S036,Robey,2023,SmoothLLM,Journal Article,arXiv preprint,https://arxiv.org/abs/2310.03684,Empirical,Randomized smoothing for LLMs,Large Language Models,GPT-family/LLaMA,Natural Language,Defense Mechanism,Character-level perturbations,N/A,SmoothLLM,Moderate (reduces ASR by 50-70%),Attack success reduction,GCG attacks,Multiple models,Randomized smoothing helps defense,Computational overhead,Improve efficiency,High - defense mechanism,5,Randomized smoothing defense
S037,Jain,2023,Baseline defenses,Journal Article,arXiv preprint,https://arxiv.org/abs/2309.00614,Empirical,Evaluate baseline defenses,Large Language Models,Vicuna/LLaMA-2,Natural Language,Defense Evaluation,Multiple baseline defenses,N/A,Various defenses,Limited effectiveness,Attack success rate,GCG attacks,Multiple models,Current defenses insufficient,Better defenses needed,Novel defense mechanisms,High - defense evaluation,5,Comprehensive defense evaluation
S038,Zhang,2024,Goal prioritization defense,Journal Article,arXiv preprint,https://arxiv.org/abs/2311.09096,Empirical,Goal prioritization for defense,Large Language Models,LLaMA-2,Natural Language,Defense Mechanism,Goal prioritization training,N/A,Goal Prioritization,Good (reduces ASR significantly),Attack success rate,AdvBench,Multiple scenarios,Goal-based defense effective,Requires careful tuning,Scale to more models,High - goal-based defense,5,Goal prioritization approach
S039,Zou,2024,Circuit breakers,Journal Article,arXiv preprint,https://arxiv.org/abs/2406.04313,Empirical,Circuit breaker defense,Large Language Models,LLaMA-family,Natural Language,Defense Mechanism,Representation engineering,N/A,Circuit Breakers,Very Good (near 100% defense),Attack success rate,Multiple benchmarks,Multiple models,Circuit breakers highly effective,Potential capability tradeoffs,Minimize capability loss,High - strong defense,5,Highly effective defense mechanism
S040,Gehman,2020,RealToxicityPrompts,Conference Paper,EMNLP Findings,https://arxiv.org/abs/2009.11462,Empirical,Evaluate toxicity in LMs,Language Models,GPT-2/GPT-3,Natural Language,Evaluation Benchmark,Toxicity measurement,N/A,None,N/A,Toxicity probability,100K prompts,100K prompts,LMs can generate toxic content,Mitigation challenging,Develop detoxification methods,High - important benchmark,5,Toxicity evaluation benchmark
S041,Perez,2023,Model-written evaluations,Conference Paper,ACL Findings,https://arxiv.org/abs/2212.09251,Empirical,Automated behavior evaluation,Large Language Models,GPT-3/InstructGPT,Natural Language,Evaluation Framework,LM-generated tests,N/A,None,N/A,Behavior detection accuracy,Multiple behavior categories,Multiple models,LMs can evaluate themselves,Evaluation quality varies,Improve evaluation,High - evaluation framework,5,Automated evaluation framework
S042,Ganguli,2022,Red teaming to reduce harms,Journal Article,arXiv preprint,https://arxiv.org/abs/2209.07858,Empirical,Red teaming methodology study,Language Models,Various sizes,Natural Language,Red Teaming,Human red teaming,Varies,RLHF,Reduces harmful outputs,Harmfulness scores,Multiple datasets,Multiple models,Red teaming scales with model size,Coverage limitations,Automated methods needed,High - methodology study,5,Comprehensive red teaming study
S043,Bai,2022,Constitutional AI,Journal Article,arXiv preprint,https://arxiv.org/abs/2212.08073,Empirical,Self-improvement for safety,Large Language Models,Claude,Natural Language,Training Method,AI feedback training,N/A,Constitutional AI,Good (improves harmlessness),Harmlessness/Helpfulness,HH-RLHF dataset,Multiple samples,Self-critique improves safety,Constitution design critical,Better constitution design,High - self-improvement,5,AI feedback for safety
S044,Ouyang,2022,Training LMs with human feedback,Conference Paper,NeurIPS,https://arxiv.org/abs/2203.02155,Empirical,Instruction following with RLHF,Language Models,GPT-3,Natural Language,Training Method,RLHF,N/A,RLHF,Good (improves alignment),Preference accuracy,InstructGPT dataset,Multiple samples,RLHF improves instruction following,Scalability challenges,Improve efficiency,High - foundational RLHF,5,InstructGPT methodology
S045,Glaese,2022,Improving dialogue alignment,Journal Article,arXiv preprint,https://arxiv.org/abs/2209.14375,Empirical,Dialogue agent alignment,Dialogue Models,Sparrow,Natural Language,Training Method,Targeted judgements,N/A,Targeted Training,Good (reduces harmful responses),Rule violation rate,Custom dataset,Multiple conversations,Targeted training effective,Tradeoffs with helpfulness,Balance safety/helpfulness,High - dialogue alignment,5,Sparrow dialogue agent
S046,Dubey,2024,Llama 3 herd of models,Technical Report,arXiv preprint,https://arxiv.org/abs/2407.21783,Technical Report,Llama 3 model development,Large Language Models,Llama 3,Natural Language,Model Development,Improved training/safety,N/A,Multiple safety mechanisms,Good (low violation rates),Various safety metrics,Multiple benchmarks,Multiple model sizes,Comprehensive safety approach,Continued research needed,Advance safety methods,High - state-of-art model,5,Latest Llama model with safety
S047,Achiam,2023,GPT-4 technical report,Technical Report,arXiv preprint,https://arxiv.org/abs/2303.08774,Technical Report,GPT-4 development,Large Language Models,GPT-4,Multi-modal,Model Development,Enhanced safety training,N/A,Multiple safety mechanisms,Good (improved over GPT-3.5),Various safety metrics,Multiple benchmarks,N/A,Significant safety improvements,Vulnerabilities remain,Continued improvement,High - state-of-art model,5,GPT-4 with enhanced safety
S048,Anil,2023,Gemini,Technical Report,arXiv preprint,https://arxiv.org/abs/2312.11805,Technical Report,Gemini model development,Multi-modal Models,Gemini,Multi-modal,Model Development,Safety considerations,N/A,Multiple safety mechanisms,Good (comprehensive safety),Various safety metrics,Multiple benchmarks,Multiple model sizes,Multi-modal safety challenges,Continued research needed,Improve multi-modal safety,High - multi-modal model,5,Multi-modal model with safety
S049,Touvron,2023,Llama 2,Technical Report,arXiv preprint,https://arxiv.org/abs/2307.09288,Technical Report,Llama 2 development,Large Language Models,Llama 2,Natural Language,Model Development,Safety fine-tuning,N/A,Multiple safety mechanisms,Good (improved safety over v1),Various safety metrics,Multiple benchmarks,Multiple model sizes,Open-source safety approach,Open challenges remain,Community safety efforts,High - open-source model,5,Open-source LLM with safety
